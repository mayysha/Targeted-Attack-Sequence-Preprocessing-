# -*- coding: utf-8 -*-
"""Targeted Attack Sequences Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mzjhdmWPeUMjVT3DpBQlW59eevTleZdz

Add the extraction.py in Files -> Upload to session storage
"""

# mount drive
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from tensorflow import keras
from keras.layers import *
from keras.models import *
from keras.utils import *
from keras.initializers import *
from keras.optimizers import * 

import tensorflow as tf
import numpy as np
import random
import csv
import extraction

# reflecting Changes
import importlib
importlib.reload(extraction)

# get attack sequences as list of lists
data_text = extraction.get_data_text()
print(data_text[0])
len(data_text)

# spitting all spaces from data_text
data_text = [[''.join(''.join(step).split()) for step in data] for data in data_text]
print(data_text[0])
len(data_text)

from collections import Counter

# get length of different sequences in a list
sequence_lengths = [len(sequence) for sequence in data_text]

counter = Counter(sequence_lengths)
print("Frequncy of sequence lengths:", counter)
print("Number of unique sequence lengths:", len(counter.keys()))
counter

def remove_sequence(data_text, lengths_to_remove):
    lengths_to_remove = lengths_to_remove
    truncated_data_text = [sequence for sequence in data_text if len(sequence) not in lengths_to_remove]
    print("First sequence after truncating:", truncated_data_text[0])
    print("Number of sequence after truncating:", len(truncated_data_text))

    return truncated_data_text

# remove sequences having length 4
lengths_to_remove = [4]
truncated_data_text = remove_sequence(data_text, lengths_to_remove)

def get_encoder_decoder_text(text, sliding_window_size, shift_right, min_decoder_length):
    encoder_text, decoder_text = [], []

    for sequence in text:
        # the window will slide 'shift_right' positions at each iteration
        for i in range(0, len(sequence) - sliding_window_size, shift_right):
            # break if decoder_text length exceeds min_decoder_length
            if i+sliding_window_size+min_decoder_length > len(sequence):
                break

            encoder_text.append(sequence[i:i+sliding_window_size])
            decoder_text.append(sequence[i+sliding_window_size:])

    return encoder_text, decoder_text

sliding_window_size = 5
shift_right = 1
min_decoder_length = 1

encoder_text, decoder_text = get_encoder_decoder_text(truncated_data_text, sliding_window_size, shift_right, min_decoder_length)
print("Number of samples:", len(encoder_text))

#tokenizing 
def append_sos_eos(decoder_text):    
    decoder_input_text, decoder_target_text = [], []

    for sequence in decoder_text:
        decoder_input_text.append(["<sos>"] + sequence[:])
        decoder_target_text.append(sequence[:] + ["<eos>"])

    return decoder_input_text, decoder_target_text

import copy
encoder_input_text = copy.deepcopy(encoder_text)
decoder_input_text, decoder_target_text = append_sos_eos(decoder_text)

print("encoder_input_text[0]:", encoder_input_text[0], "\ndecoder_input_text[0]:", decoder_input_text[0], "\ndecoder_target_text[0]:", decoder_target_text[0], "\n")
#Text sequences to integer sequences

from keras.preprocessing.text import Tokenizer

def get_tokenizer(text):
    tokenizer = Tokenizer(num_words=200, lower=False) # we just give a large enough arbitrary number
    tokenizer.fit_on_texts(text)
    
    # builid word2idx and idx2word dictionary
    word2idx = copy.deepcopy(tokenizer.word_index)
    idx2word = {v:k for k, v in tokenizer.word_index.items()}

    return tokenizer, word2idx, idx2word

def get_encoder_decoder_indices(encoder_input_text, decoder_input_text, decoder_target_text):
    encoder_tokenizer, encoder_word2idx, encoder_idx2word = get_tokenizer(encoder_input_text)
    print("encoder_word2idx:", encoder_word2idx)
    print("encoder_idx2word:", encoder_idx2word)
    encoder_input_indices = encoder_tokenizer.texts_to_sequences(encoder_input_text)


    decoder_tokenizer, decoder_word2idx, decoder_idx2word = get_tokenizer(decoder_input_text + decoder_target_text)
    print("\ndecoder_word2idx:", decoder_word2idx)
    print("decoder_idx2word:", decoder_idx2word)
    decoder_input_indices = decoder_tokenizer.texts_to_sequences(decoder_input_text)
    decoder_target_indices = decoder_tokenizer.texts_to_sequences(decoder_target_text)

    print("\nencoder_input_indices[0]:", encoder_input_indices[0], "\ndecoder_input_indices[0]:", decoder_input_indices[0], "\ndecoder_target_indices[0]:", decoder_target_indices[0], "\n")
    print("encoder_input_indices[69]:", encoder_input_indices[69], "\ndecoder_input_indices[69]:", decoder_input_indices[69], "\ndecoder_target_indices[69]:", decoder_target_indices[69], "\n")
    print("encoder_input_indices[169]:", encoder_input_indices[169], "\ndecoder_input_indices[169]:", decoder_input_indices[169], "\ndecoder_target_indices[169]:", decoder_target_indices[169], "\n")
    print("encoder_input_indices[650]:", encoder_input_indices[650], "\ndecoder_input_indices[650]:", decoder_input_indices[650], "\ndecoder_target_indices[650]:", decoder_target_indices[650], "\n")

    return encoder_input_indices, decoder_input_indices, decoder_target_indices, encoder_word2idx, encoder_idx2word, decoder_word2idx, decoder_idx2word

# get input_indices and word conversion dicts
encoder_input_indices, decoder_input_indices, decoder_target_indices, encoder_word2idx, encoder_idx2word, decoder_word2idx, decoder_idx2word \
= \
get_encoder_decoder_indices(encoder_input_text, decoder_input_text, decoder_target_text)
#Padding, Split & One-hot **

max_encoder_seq_length = max([len(sequence) for sequence in encoder_input_indices])
max_decoder_seq_length = max([len(sequence) for sequence in decoder_input_indices])

print("Max sequence length for encoder:", max_encoder_seq_length)
print("Max sequence length for decoder:", max_decoder_seq_length)

from keras.preprocessing.sequence import pad_sequences

def get_padded_inputs(encoder_input_indices, decoder_input_indices, decoder_target_indices):
    
    padded_decoder_input = pad_sequences(decoder_input_indices, maxlen=max_decoder_seq_length, dtype='int32', padding='post')
    padded_decoder_target = pad_sequences(decoder_target_indices, maxlen=max_decoder_seq_length, dtype='int32', padding='post')
    print("\npadded_decoder_input[0]", padded_decoder_input[0])
    print("padded_decoder_target[0]", padded_decoder_target[0])
    print("padded_decoder_input[69]", padded_decoder_input[69])
    print("padded_decoder_target[69", padded_decoder_target[69])

    return padded_decoder_input, padded_decoder_target

padded_decoder_input, padded_decoder_target = get_padded_inputs(encoder_input_indices, decoder_input_indices, decoder_target_indices)